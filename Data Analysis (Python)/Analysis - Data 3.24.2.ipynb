{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'twtgun3.24.1.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5add867f8691>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'twtgun3.24.'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.txt'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfhand\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfhand\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mdic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'twtgun3.24.1.txt'"
     ]
    }
   ],
   "source": [
    "# Before run this codes, increase data rate by jupyter notebook --NotebookApp.iopub_data_rate_limit=1.0e10\n",
    "\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib notebook\n",
    "pd.options.display.max_rows = 150\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "#create the list of dictinary formatted strings by extracting data item\n",
    "#from twitter API scrape\n",
    "#files = ['twtgun3.24.1.txt', 'twtgun3.24.2.txt','twtgun3.24.3.txt', 'twtgun3.24.4.txt']\n",
    "\n",
    "group = []\n",
    "for i in range(1,11):\n",
    "   \n",
    "    with open('twtgun3.24.' + str(i) + '.txt') as fhand:\n",
    "        for item in fhand:\n",
    "            dic = {}\n",
    "            x = re.findall(\"^{'created_at': \\'(.*?)\\'\", item)\n",
    "            if len(x) > 0:\n",
    "                dic[\"created_at\"] = str(x[0])\n",
    "            else:\n",
    "                dic[\"created_at\"] = None\n",
    "            y = re.findall(\"{'full_text': \\'(.*?)\\'\", item)\n",
    "            if len(y) > 0:\n",
    "                dic[\"full_text\"] = str(y[0])\n",
    "            else:\n",
    "                dic[\"full_text\"] = None\n",
    "            z = re.findall(\" 'text': \\'(.*?)\\'\", item)\n",
    "            if len(z) > 0:\n",
    "                dic[\"text\"] = str(z[0])\n",
    "            else:\n",
    "                dic[\"text\"] = None\n",
    "            a = re.findall(\"\\'location\\': \\'(.*?)\\'\", item)\n",
    "            if len(a) > 0:\n",
    "                dic[\"location\"] = str(a[0])\n",
    "            else:\n",
    "                dic[\"location\"] = None\n",
    "            b = re.findall(\"\\'time_zone\\': \\'(.*?)\\'\", item)\n",
    "            if len(b) > 0:\n",
    "                dic[\"time_zone\"] = str(b[0])\n",
    "            else:\n",
    "                dic[\"time_zone\"] = None\n",
    "            group.append(dic)\n",
    "\n",
    "    fhand.close()\n",
    "\n",
    "#convert the list into json object\n",
    "\n",
    "with open('data.json', 'w') as fp:\n",
    "    json.dump(group, fp)\n",
    "\n",
    "# convert json file into pandas DataFrame\n",
    "df = pd.read_json('data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.sample(5)  # To check the data\n",
    "#df.to_csv('df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2 = df.reindex(columns = ['created_at','location','time_zone','text','full_text'])\n",
    "pd.options.display.max_rows=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2['text'] = df2['text'].replace(r'\\\\n', '', regex=True).str.lower()\n",
    "df2['text'] = df2['text'].str.strip()\n",
    "df2['full_text'] = df2['full_text'].replace(r'\\\\n', '', regex=True).str.lower()\n",
    "df2['full_text'] = df2['full_text'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(df2.head())    # To check the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df2['full_text'].str.findall('\\\\n') just to make sure no data will be found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make copy of df2 and name it df3\n",
    "df3 = df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Take 'full_text' when available, otherwise take 'text'\n",
    "df3['tweet'] = df3['full_text'].where(pd.notna(df3['full_text']), other = df3['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.isna(df3['tweet']).sum()  # Check how many 'tweet' observations are missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#drop columns 'text' & 'full_text'\n",
    "df3 = df3.drop(['text', 'full_text'], axis=1)\n",
    "#Extract hashtag from 'tweet' and create a new cloumn 'hashtag'\n",
    "df3['hashtag'] = df3['tweet'].str.findall(r'#.*?\\s')  # .findall returns list which causes issues later\n",
    "df3.reindex(columns = ['created_at','location','time_zone','tweet', 'hashtag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#drop rows where all columns are NaN\n",
    "df3 = df3.dropna(how='all')\n",
    "print(df3.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert'created_at' time data rounding to nearest minute \n",
    "df3['created_at'] = df3['created_at'].apply(lambda x: x.round('min'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check how many tweets creatd every minute during the data collection period\n",
    "# At this point I just want to check the time trend of the tweets which can be done without time-zone conversion.\n",
    "df3.groupby('created_at').count()['tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#UTC is 5 hours earlier than EST. So in the graph below, tweets were peaked around 15:30pm\n",
    "df3.groupby('created_at').count()['tweet'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# As the next step, use regular expression to tokenize tweets\n",
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    " \n",
    "regex_str = [\n",
    "    emoticons_str,\n",
    "    r'<[^>]+>', # HTML tags\n",
    "    r'(?:@[\\w_]+)', # @-mentions\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\", # words with - and '\n",
    "    r'(?:[\\w_]+)', # other words\n",
    "    r'(?:\\S)' # anything else\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE)\n",
    " \n",
    "def tokenize(s):\n",
    "    return tokens_re.findall(s)\n",
    " \n",
    "def preprocess(s, lowercase=False):\n",
    "    tokens = tokenize(s)\n",
    "    if lowercase:\n",
    "        tokens = [token if emoticon_re.search(token) else token.lower() for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test run using only one tweet\n",
    "tweet = df3['tweet'][10]\n",
    "print(preprocess(tweet))   #success!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a new dataframe by dropping rows with NA data\n",
    "df4 = df3.dropna(axis=0, inplace=False)\n",
    "df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Tokenize tweet data in the dataframe\n",
    "df4['preprocess_tweet'] = df4['tweet'].apply(lambda x : preprocess(x))\n",
    "df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from nltk download premade stopwords list\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "punc = list(string.punctuation)\n",
    "stop = stopwords.words('english') + punc + ['rt', 'via', '’', 'amp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Test run using only one tweet\n",
    "terms_nostop = [term for term in preprocess(tweet) if term not in stop]  #success!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# There are 37487 rows in df4. for each item (list) in the 'preprocess_tweet' column, check if it's non-empty, then \n",
    "# remove stop words from the item and append it to the longer 'items' list. Since the data is large, \n",
    "# increase data rate by jupyter notebook --NotebookApp.iopub_data_rate_limit=1.0e10\n",
    "\n",
    "items = []\n",
    "for item in df4['preprocess_tweet']:\n",
    "    if len(item) !=0:\n",
    "        for i in item:\n",
    "            if i not in stop and not i.startswith(('#', '@')):\n",
    "                items.append(i.strip())\n",
    "            else:\n",
    "                continue  \n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "print(items[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Listing top 30 most common words in tweets during the data collection period\n",
    "import operator\n",
    "from collections import Counter\n",
    "\n",
    "count_all = Counter()\n",
    "count_all.update(items)\n",
    "print(count_all.most_common(30))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Creating the list of hashtags tweeted during the data colletion period\n",
    "hashlist = []\n",
    "for item in df4['hashtag']:\n",
    "    if len(item) !=0:\n",
    "        for i in item:\n",
    "            if i.startswith('#'):\n",
    "                hashlist.append(i.strip())\n",
    "            else:\n",
    "                continue  \n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "print(hashlist[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#control_pattern = re.compile(r'#guncontrol[\\.]|#guncont[r]?ol[\\w!]+|#gunreform[.\\w]*')\n",
    "control_pattern = re.compile(r'#guncont[r]?ol[\\w!]+|#gunreform[.\\w]*')                                \n",
    "clean_hashlist =[]\n",
    "for item in hashlist:\n",
    "    i = control_pattern.sub(r'#guncontrol', item)\n",
    "    clean_hashlist.append(i)\n",
    "    \n",
    "march_pattern = re.compile(r'#march4ourlives|#marchforourlives!|#marchforourlives[\\w.]+')\n",
    "\n",
    "clean_hashlist2 =[]\n",
    "for item in clean_hashlist:\n",
    "    i = march_pattern.sub(r'#marchforourlives', item)\n",
    "    clean_hashlist2.append(i)\n",
    "    \n",
    "enough_pattern = re.compile(r'#enough[.\\w]+')\n",
    "\n",
    "clean_hashlist3 =[]\n",
    "for item in clean_hashlist2:\n",
    "    i = enough_pattern.sub(r'#enough', item)\n",
    "    clean_hashlist3.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Listing top 30 most popular hash tags during the data collection period\n",
    "\n",
    "count_hash = Counter()\n",
    "count_hash.update(clean_hashlist3)\n",
    "print(count_hash.most_common(30))\n",
    "\n",
    "# \"#guncontrol', '#guncontrol.', and '#guncontrolnow' should be count toghether. So should '#marchforourlives' and '#march4ourlives' etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to remove emojis from tokens, compile emoji patterns to be removed- Unicode some emoji code -shorter ones- cannot\n",
    "# be compiled. Why?\n",
    "\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F500\"  # symbols & pictographs\n",
    "        u\"\\U0001F520-\\U0001F52F\"\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)                          \n",
    "        u\"\\U0001F910-\\U0001F96B\" \n",
    "        u\"\\U0001F52B\"\n",
    "        u\"\\U0001F5E3\"             # speech\n",
    "        u\"\\U0001F5F3\"             # vote \n",
    "        u\"\\U0001F91B-\\U0001F939\"                   \n",
    "        u\"\\U0001F191-\\U0001F19A\"\n",
    "        u\"\\U0001F595\"\n",
    "                             \"]+\", flags = re.UNICODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove emojis from the list of tokenized words\n",
    "items_noemoji = []\n",
    "for item in items:\n",
    "    i = emoji_pattern.sub(r'', item)\n",
    "    items_noemoji.append(i)\n",
    "    \n",
    "print(items_noemoji[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Listing top 30 word-pairs tweeted together during the data collection periods\n",
    "from nltk import bigrams\n",
    "\n",
    "terms_bigram = list(bigrams(items_noemoji))\n",
    "count_bigram = Counter()\n",
    "count_bigram.update(terms_bigram)\n",
    "print(count_bigram.most_common(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Visualize top 30 popular hashtags\n",
    "\n",
    "import vincent\n",
    "vincent.core.initialize_notebook()\n",
    "\n",
    "hash_freq = count_hash.most_common(30)  #create list of tuples\n",
    "labels, freq = zip(*hash_freq)  #seperate the above into 1. tuple of labels & 2. tuple of counts\n",
    "data = {'data': freq, 'x': labels} #create dictionary of tuples\n",
    "bar = vincent.Bar(data, iter_idx='x')\n",
    "bar.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create the list of time stamps when #hermosabeach was tweeted\n",
    "beach_hash_time =[]\n",
    "\n",
    "for i in range(len(df4['tweet'])):\n",
    "    if '#hermosabeach' in df4['tweet'].iloc[i]:\n",
    "        beach_hash_time.append(df4['created_at'].iloc[i])\n",
    "        \n",
    "print(beach_hash_time[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating datetimeindex for time series data for pandas. Twitter streaming data is based on UTC time.\n",
    "# From US & Canada ETS time, it's 4 hours earlier. Adjust for daytime savings.\n",
    "\n",
    "from pandas.tseries.offsets import Hour\n",
    "one_hour = Hour(1)\n",
    "\n",
    "idx = pd.DatetimeIndex(beach_hash_time)\n",
    "idx_local = idx.tz_localize(tz='UTC')\n",
    "idx_est = idx_local.tz_convert(tz='US/Eastern')\n",
    "idx_est_ds = idx_est - one_hour    #daytime savings adjustment\n",
    "ones = [1]*len(beach_hash_time)\n",
    "#ones\n",
    "beach_hash = pd.Series(ones, index=idx_est_ds)\n",
    "per_minute = beach_hash.resample('1min').sum().fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Visualize the frequency of #hermosabeach was tweeted during the data collection period\n",
    "time_chart = vincent.Line(per_minute)\n",
    "time_chart.axis_titles(x='Time', y='Hashtag frequencies')\n",
    "time_chart.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepar hashtag #marchforourlives for the same analysis\n",
    "march_hash_time =[]\n",
    "\n",
    "for i in range(len(df4['tweet'])):\n",
    "    if '#marchforourlives' in df4['tweet'].iloc[i]:\n",
    "        march_hash_time.append(df4['created_at'].iloc[i])\n",
    "        \n",
    "idx = pd.DatetimeIndex(march_hash_time)\n",
    "idx_local = idx.tz_localize(tz='UTC')\n",
    "idx_est = idx_local.tz_convert(tz='US/Eastern')\n",
    "idx_est_ds = idx_est - one_hour    #daytime savings adjustment\n",
    "ones = [1]*len(march_hash_time)\n",
    "#ones\n",
    "march_hash = pd.Series(ones, index=idx_est_ds)\n",
    "march_per_minute = march_hash.resample('1min').sum().fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepar hashtag #enough for the same analysis\n",
    "enough_hash_time =[]\n",
    "\n",
    "for i in range(len(df4['tweet'])):\n",
    "    if '#enough' in df4['tweet'].iloc[i]:\n",
    "        enough_hash_time.append(df4['created_at'].iloc[i])\n",
    "        \n",
    "idx = pd.DatetimeIndex(enough_hash_time)\n",
    "idx_local = idx.tz_localize(tz='UTC')\n",
    "idx_est = idx_local.tz_convert(tz='US/Eastern')\n",
    "idx_est_ds = idx_est - one_hour    #daytime savings adjustment\n",
    "ones = [1]*len(enough_hash_time)\n",
    "#ones\n",
    "enough_hash = pd.Series(ones, index=idx_est_ds)\n",
    "enough_per_minute = enough_hash.resample('1min').sum().fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepar hashtag #neveragain for the same analysis\n",
    "again_hash_time =[]\n",
    "\n",
    "for i in range(len(df4['tweet'])):\n",
    "    if '#neveragain' in df4['tweet'].iloc[i]:\n",
    "        again_hash_time.append(df4['created_at'].iloc[i])\n",
    "        \n",
    "idx = pd.DatetimeIndex(again_hash_time)\n",
    "idx_local = idx.tz_localize(tz='UTC')\n",
    "idx_est = idx_local.tz_convert(tz='US/Eastern')\n",
    "idx_est_ds = idx_est - one_hour    #daytime savings adjustment\n",
    "ones = [1]*len(again_hash_time)\n",
    "#ones\n",
    "again_hash = pd.Series(ones, index=idx_est_ds)\n",
    "again_per_minute = again_hash.resample('1min').sum().fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepar hashtag #guncontrol for the same analysis\n",
    "control_hash_time =[]\n",
    "\n",
    "for i in range(len(df4['tweet'])):\n",
    "    if '#guncontrol' in df4['tweet'].iloc[i]:\n",
    "        control_hash_time.append(df4['created_at'].iloc[i])\n",
    "        \n",
    "idx = pd.DatetimeIndex(control_hash_time)\n",
    "idx_local = idx.tz_localize(tz='UTC')\n",
    "idx_est = idx_local.tz_convert(tz='US/Eastern')\n",
    "idx_est_ds = idx_est - one_hour    #daytime savings adjustment\n",
    "ones = [1]*len(control_hash_time)\n",
    "#ones\n",
    "control_hash = pd.Series(ones, index=idx_est_ds)\n",
    "control_per_minute = control_hash.resample('1min').sum().fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepar hashtag #gunviolence for the same analysis\n",
    "violence_hash_time =[]\n",
    "\n",
    "for i in range(len(df4['tweet'])):\n",
    "    if '#gunviolence' in df4['tweet'].iloc[i]:\n",
    "        violence_hash_time.append(df4['created_at'].iloc[i])\n",
    "        \n",
    "idx = pd.DatetimeIndex(violence_hash_time)\n",
    "idx_local = idx.tz_localize(tz='UTC')\n",
    "idx_est = idx_local.tz_convert(tz='US/Eastern')\n",
    "idx_est_ds = idx_est - one_hour    #daytime savings adjustment\n",
    "ones = [1]*len(violence_hash_time)\n",
    "\n",
    "violence_hash = pd.Series(ones, index=idx_est_ds)\n",
    "violence_per_minute = violence_hash.resample('1min').sum().fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepar hashtag #schoolshooting for the same analysis\n",
    "school_hash_time =[]\n",
    "\n",
    "for i in range(len(df4['tweet'])):\n",
    "    if '#schoolshooting' in df4['tweet'].iloc[i]:\n",
    "        school_hash_time.append(df4['created_at'].iloc[i])\n",
    "        \n",
    "idx = pd.DatetimeIndex(school_hash_time)\n",
    "idx_local = idx.tz_localize(tz='UTC')\n",
    "idx_est = idx_local.tz_convert(tz='US/Eastern')\n",
    "idx_est_ds = idx_est - one_hour    #daytime savings adjustment\n",
    "ones = [1]*len(school_hash_time)\n",
    "#ones\n",
    "school_hash = pd.Series(ones, index=idx_est_ds)\n",
    "school_per_minute = school_hash.resample('1min').sum().fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Visualize the frequency of #shoolshooting was tweeted\n",
    "time_chart = vincent.Line(school_per_minute)\n",
    "time_chart.axis_titles(x='Time', y='Hashtag frequencies')\n",
    "time_chart.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "match_data = dict(Beach=per_minute, March=march_per_minute, Enough=enough_per_minute, Again=again_per_minute, Control=control_per_minute, Violence=violence_per_minute)\n",
    "all_matches = pd.DataFrame(data = match_data,index=march_per_minute.index)\n",
    "all_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Visualize and comparing the frequencies of the top 3 the most popular hashtags during the data collection time\n",
    "time_chart_top3 = vincent.Line(all_matches[['March', 'Enough', 'Control']])\n",
    "time_chart_top3.axis_titles(x='Time', y='Freq')\n",
    "time_chart_top3.legend(title='Top 3 Most popular hashtags')\n",
    "time_chart_top3.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Visualize and comparing the frequencies of the next 3 of the most popular hashtags during the data collection time\n",
    "time_chart_next3 = vincent.Line(all_matches[['Violence', 'Again', 'Beach']])\n",
    "time_chart_next3.axis_titles(x='Time', y='Freq')\n",
    "time_chart_next3.legend(title='Top 4 to 6 Most popular hashtags')\n",
    "time_chart_next3.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Analyze number of tweets by geographic locations. Within USA, broken down by states\n",
    "df5 = df4[(df4['time_zone']=='Pacific Time (US & Canada)')|(df4['time_zone']=='Eastern Time (US & Canada)')|(df4['time_zone']=='Central Time (US & Canada)')|(df4['time_zone']=='Mountain Time (US & Canada)')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df5['processed_location'] = df5['location'].apply(lambda x: emoji_pattern.sub(r'', x))\n",
    "hashtag = re.compile(r'(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)')\n",
    "number = re.compile(r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)')\n",
    "\n",
    "df5['processed_location2'] = df5['processed_location'].apply(lambda x: hashtag.sub(r'', x))\n",
    "df5['processed_location3'] = df5['processed_location2'].apply(lambda x: number.sub(r'', x))\n",
    "df5 = df5.drop(['processed_location2', 'processed_location'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pattern(text):\n",
    "    x = re.compile(r'[\\w\\s]+[\\s]?[\\w]+, ([A-Z][A-Z]+)')\n",
    "    match = x.search(text)   #return match object\n",
    "    if match != None:\n",
    "        return match.group(1)\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df5['clean_states'] = df5['processed_location3'].apply(lambda text: pattern(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pattern2(text):\n",
    "    x = re.compile(r'([\\w\\s]+[\\s]?[\\w]+), US[A]?|[\\w]+,(Texas), EE.UU.|[\\w\\s]+[\\w]+,(Florida)|(NH)\\s USA|Cape\\s Cod,(Ma)|Hastings,\\s(NE)B|(NY),way|Athens,\\s(Georgia)')\n",
    "    match = x.search(text)\n",
    "    if match != None:\n",
    "        return match.group(1)\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df5['clean_states2'] = df5['processed_location3'].apply(lambda text : pattern2(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df5['clean_states'].to_csv('name2.csv')\n",
    "#df5['clean_states2'].to_csv('name4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = {'Alabama':'AL', 'Alaska':'AK', 'Arizona':'AZ','Arkansas':'AR', 'California':'CA', 'Colorado':'CO', 'Connecticut':'CT', 'Delaware':'DE', 'Florida':'FL', 'Georgia':'GA', 'Hawaii':'HI', 'Idaho':'ID', 'Illinois':'IL', 'Indiana':'IN', 'Iowa':'IA', 'Kansas':'KS', 'Kentucky':'KY', 'Louisiana':'LA', 'Maine':'ME', 'Maryland':'MD', 'Massachusetts':'MA', 'Michigan':'MI', 'Minnesota':'MN', 'Mississippi':'MS', 'Missouri':'MO', 'Montana':'MT', 'Nebraska':'NE', 'Nevada':'NV', 'New Hampshire':'NH', 'New Jersey':'NJ', 'New Mexico':'NM', 'New York':'NY', 'North Carolina':'NC', 'North Dakota':'ND', 'Ohio':'OH', 'Oklahoma':'OK', 'Oregon':'OR', 'Pennsylvania':'PA', 'Rhode Island':'RI', 'South Carolina':'SC', 'Tennessee':'TN', 'Texas':'TX', 'Utah':'UT', 'Vermont':'VT', 'Virginia':'VA', 'Washington':'WA', 'West Virginia':'WV', 'Wisconsin':'WI', 'Wyoming':'WY', 'American Samoa':'AS', 'District of Columbia':'DC', 'Federated States of Micronesia':'FM', 'Guam':'GU', 'Marshall Islands':'MH', 'Northern Mariana Islands':'MP', 'Palau':'PW', 'Puerto Rico':'PR', 'Virgin Islands':'VI'}\n",
    "df5['clean_states2'] = df5['clean_states2'].str.strip()\n",
    "df5['clean_states2'] = df5['clean_states2'].replace(d)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df5['clean_states2']=df5['clean_states2'].replace(to_replace=[r'[\\w]+\\sNew\\sYork', r'DC\\s[\\w\\s]+', r'[\\w]+\\sKentucky', r'[\\w]+\\sWisconsin', r'[\\w]+\\sTexas', r'[\\w]+\\sCalifornia', r'[\\w]+\\sFlorida',r'[\\w]+\\sSC',r'[\\w]+\\sMichigan',r'[\\w]+\\sIndiana',r'[\\w\\s]+\\sNew\\sJersey',r'[\\w]+\\sWashington\\sState',r'Virginia\\sand\\sFlorida',r'[\\w\\s]*Jersey\\sShore'], value=['NY','DC','KY','WI','TX','CA','FL','SC','MI','IN','NJ','WA','FL','NJ'], regex=True)\n",
    "df5['clean_states2']=df5['clean_states2'].replace(to_replace =['TEXAS','Northern Virginia','Nueva York','ARIZONA','Tenn','South Dakota'], value=['TX','VA','NY','AZ','TN','SD'])\n",
    "df5['clean_states2']=df5['clean_states2'].replace(to_replace =['Boston','Albuquerque','Dallas','Seattle','Philly','Brooklyn','Phoenix'], value=['MA','NM','TX','WA','PA','NY','AZ'])\n",
    "df5['clean_states2']=df5['clean_states2'].replace(to_replace =['West Coast','Southeast','New England','Midwest','Left Coast','Canada'], value=['','','','','',''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df5['states'] = np.where(df5['clean_states'] !=\"USA\", df5['clean_states'], df5['clean_states2'])\n",
    "df5['states']=df5['states'].replace(to_replace =['CALIFORNIA','EE','GODS','NOT','NEB','OHIO','TEXAS'], value=['CA','TX','NH','NY','NE','OH','TX'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df5.to_csv('text.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
