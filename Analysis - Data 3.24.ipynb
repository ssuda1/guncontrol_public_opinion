{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "\n",
    "#create the list of dictinary formatted strings by extracting data item\n",
    "#from twitter API scrape\n",
    "#files = ['twtgun3.24.1.txt', 'twtgun3.24.2.txt','twtgun3.24.3.txt', 'twtgun3.24.4.txt']\n",
    "\n",
    "group = []\n",
    "for i in range(1,11):\n",
    "   \n",
    "    with open('twtgun3.24.' + str(i) + '.txt') as fhand:\n",
    "        for item in fhand:\n",
    "            dic = {}\n",
    "            x = re.findall(\"^{'created_at': \\'(.*?)\\'\", item)\n",
    "            if len(x) > 0:\n",
    "                dic[\"created_at\"] = str(x[0])\n",
    "            else:\n",
    "                dic[\"created_at\"] = None\n",
    "            y = re.findall(\"{'full_text': \\'(.*?)\\'\", item)\n",
    "            if len(y) > 0:\n",
    "                dic[\"full_text\"] = str(y[0])\n",
    "            else:\n",
    "                dic[\"full_text\"] = None\n",
    "            z = re.findall(\" 'text': \\'(.*?)\\'\", item)\n",
    "            if len(z) > 0:\n",
    "                dic[\"text\"] = str(z[0])\n",
    "            else:\n",
    "                dic[\"text\"] = None\n",
    "            a = re.findall(\"\\'location\\': \\'(.*?)\\'\", item)\n",
    "            if len(a) > 0:\n",
    "                dic[\"location\"] = str(a[0])\n",
    "            else:\n",
    "                dic[\"location\"] = None\n",
    "            b = re.findall(\"\\'time_zone\\': \\'(.*?)\\'\", item)\n",
    "            if len(b) > 0:\n",
    "                dic[\"time_zone\"] = str(b[0])\n",
    "            else:\n",
    "                dic[\"time_zone\"] = None\n",
    "            group.append(dic)\n",
    "\n",
    "    fhand.close()\n",
    "\n",
    "#convert the list into json object\n",
    "\n",
    "with open('data.json', 'w') as fp:\n",
    "    json.dump(group, fp)\n",
    "\n",
    "    # convert json file into pandas DataFrame\n",
    "df = pd.read_json('data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.reindex(columns = ['created_at','location','time_zone','text','full_text'])\n",
    "pd.options.display.max_rows=10\n",
    "#print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['text'] = df2['text'].replace(r'\\\\n', '', regex=True).str.lower()\n",
    "df2['text'] = df2['text'].str.strip()\n",
    "df2['full_text'] = df2['full_text'].replace(r'\\\\n', '', regex=True).str.lower()\n",
    "df2['full_text'] = df2['full_text'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2['full_text'].str.findall('\\\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make copy of df2 and name it df3\n",
    "df3 = df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take 'full_text' when available, otherwise take 'text'\n",
    "df3['tweet'] = df3['full_text'].where(pd.notna(df3['full_text']), other = df3['text'])\n",
    "#Use time_zone for proxy of location when location data is missing\n",
    "df3['location_'] = df3['location'].where(pd.notna(df3['location']), other = df3['time_zone'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.isna(df3['tweet']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop columns 'text' & 'full_text'\n",
    "df3 = df3.drop(['text', 'full_text', 'location', 'time_zone'], axis=1)\n",
    "df3.rename(columns = {'location_':'location'}, inplace = True)\n",
    "df3.reindex(columns = ['created_at','location','tweet'])\n",
    "\n",
    "#drop rows all elements are NaN\n",
    "\n",
    "df3 = df3.dropna(how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename locations taken from time_zone ex) Eastern Time (US & Canada) -> US & Canada EastCoast etc \n",
    "#df3.to_csv('test2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Etract hashtag from 'tweet' and create a new cloumn 'hashtag'\n",
    "df3['hashtag'] = df3['tweet'].str.findall(r'#.*?\\s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df3.reindex(columns = ['created_at','location','tweet','hashtag'])\n",
    "print(df3.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['created_at'] = df3['created_at'].apply(lambda x: x.round('min'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.groupby('created_at').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.groupby('created_at').count()['tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.groupby('created_at').count()['tweet'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    " \n",
    "regex_str = [\n",
    "    emoticons_str,\n",
    "    r'<[^>]+>', # HTML tags\n",
    "    r'(?:@[\\w_]+)', # @-mentions\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    " \n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\", # words with - and '\n",
    "    r'(?:[\\w_]+)', # other words\n",
    "    r'(?:\\S)' # anything else\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE)\n",
    " \n",
    "def tokenize(s):\n",
    "    return tokens_re.findall(s)\n",
    " \n",
    "def preprocess(s, lowercase=False):\n",
    "    tokens = tokenize(s)\n",
    "    if lowercase:\n",
    "        tokens = [token if emoticon_re.search(token) else token.lower() for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = df3['tweet'][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preprocess(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df3.dropna(axis=0, inplace=False)\n",
    "df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df4['test'] = df4['tweet'].apply(lambda x: type(x))\n",
    "#df4.to_csv('test3.csv')\n",
    "df4.loc[:,'preprocess_tweet'] = df4.loc[:,'tweet'].apply(lambda x : preprocess(x))\n",
    "df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "punc = list(string.punctuation)\n",
    "stop = stopwords.words('english') + punc + ['rt', 'via', 'â€™', 'amp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_nostop = [term for term in preprocess(tweet) if term not in stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are 37487 rows in df4. for each item (list) in the 'preprocess_tweet' column, check if it's non-empty, them \n",
    "# remove stop words from the item and append it to the longer 'items' list. Since the data is large, will run the loop\n",
    "# for 4 times (10,000 x 3 + 7487 X 1)\n",
    "# increase data rate by jupyter notebook --NotebookApp.iopub_data_rate_limit=1.0e10\n",
    "\n",
    "items = []\n",
    "for item in df4['preprocess_tweet']:\n",
    "    if len(item) !=0:\n",
    "        for i in item:\n",
    "            if i not in stop and not i.startswith(('#', '@')):\n",
    "                items.append(i.strip())\n",
    "            else:\n",
    "                continue  \n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "print(items[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from collections import Counter\n",
    "\n",
    "count_all = Counter()\n",
    "count_all.update(items)\n",
    "print(count_all.most_common(30))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashlist = []\n",
    "for item in df4['hashtag']:\n",
    "    if len(item) !=0:\n",
    "        for i in item:\n",
    "            if i.startswith('#'):\n",
    "                hashlist.append(i.strip())\n",
    "            else:\n",
    "                continue  \n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "print(hashlist[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_hash = Counter()\n",
    "count_hash.update(hashlist)\n",
    "print(count_hash.most_common(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to remove emojis from tokens\n",
    "\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F500\"  # symbols & pictographs\n",
    "        u\"\\U0001F520-\\U0001F52F\"\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)                          \n",
    "        u\"\\U0001F910-\\U0001F96B\" \n",
    "        u\"\\U0001F52B\"\n",
    "        u\"\\U0001F5E3\"             # speech\n",
    "        u\"\\U0001F5F3\"             # vote \n",
    "        u\"\\U0001F91B-\\U0001F939\"                   \n",
    "        u\"\\U0001F191-\\U0001F19A\"\n",
    "        u\"\\U0001F595\"\n",
    "                             \"]+\", flags = re.UNICODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_noemoji = []\n",
    "for item in items:\n",
    "    i = emoji_pattern.sub(r'', item)\n",
    "    items_noemoji.append(i)\n",
    "    \n",
    "print(items_noemoji[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import bigrams\n",
    "\n",
    "terms_bigram = list(bigrams(items_noemoji))\n",
    "count_bigram = Counter()\n",
    "count_bigram.update(terms_bigram)\n",
    "print(count_bigram.most_common(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vincent\n",
    "vincent.core.initialize_notebook()\n",
    "\n",
    "hash_freq = count_hash.most_common(30)  #create list of tuples\n",
    "labels, freq = zip(*hash_freq)  #seperate the above into 1. tuple of labels & 2. tuple of counts\n",
    "data = {'data': freq, 'x': labels} #create dictionary of tuples\n",
    "bar = vincent.Bar(data, iter_idx='x')\n",
    "bar.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "beach_hash_time =[]\n",
    "\n",
    "for i in range(len(df4['tweet'])):\n",
    "    if '#hermosabeach' in df4['tweet'].iloc[i]:\n",
    "        beach_hash_time.append(df4['created_at'].iloc[i])\n",
    "        \n",
    "print(beach_hash_time[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating datetimeindex for time series data for pandas. Twitter streaming data is based on UTC time.\n",
    "# From US & Canada ETS time, it's 4 hours earlier. Adjust for daytime savings.\n",
    "\n",
    "from pandas.tseries.offsets import Hour\n",
    "one_hour = Hour(1)\n",
    "\n",
    "idx = pd.DatetimeIndex(beach_hash_time)\n",
    "idx_local = idx.tz_localize(tz='UTC')\n",
    "idx_est = idx_local.tz_convert(tz='US/Eastern')\n",
    "idx_est_ds = idx_est - one_hour    #daytime savings adjustment\n",
    "ones = [1]*len(beach_hash_time)\n",
    "#ones\n",
    "beach_hash = pd.Series(ones, index=idx_est_ds)\n",
    "per_minute = beach_hash.resample('1min').sum().fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_chart = vincent.Line(per_minute)\n",
    "time_chart.axis_titles(x='Time', y='Hashtag frequencies')\n",
    "time_chart.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "march_hash_time =[]\n",
    "\n",
    "for i in range(len(df4['tweet'])):\n",
    "    if '#marchforourlives' in df4['tweet'].iloc[i]:\n",
    "        march_hash_time.append(df4['created_at'].iloc[i])\n",
    "        \n",
    "idx = pd.DatetimeIndex(march_hash_time)\n",
    "idx_local = idx.tz_localize(tz='UTC')\n",
    "idx_est = idx_local.tz_convert(tz='US/Eastern')\n",
    "idx_est_ds = idx_est - one_hour    #daytime savings adjustment\n",
    "ones = [1]*len(march_hash_time)\n",
    "#ones\n",
    "march_hash = pd.Series(ones, index=idx_est_ds)\n",
    "march_per_minute = march_hash.resample('1min').sum().fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enough_hash_time =[]\n",
    "\n",
    "for i in range(len(df4['tweet'])):\n",
    "    if '#enough' in df4['tweet'].iloc[i]:\n",
    "        enough_hash_time.append(df4['created_at'].iloc[i])\n",
    "        \n",
    "idx = pd.DatetimeIndex(enough_hash_time)\n",
    "idx_local = idx.tz_localize(tz='UTC')\n",
    "idx_est = idx_local.tz_convert(tz='US/Eastern')\n",
    "idx_est_ds = idx_est - one_hour    #daytime savings adjustment\n",
    "ones = [1]*len(enough_hash_time)\n",
    "#ones\n",
    "enough_hash = pd.Series(ones, index=idx_est_ds)\n",
    "enough_per_minute = enough_hash.resample('1min').sum().fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "again_hash_time =[]\n",
    "\n",
    "for i in range(len(df4['tweet'])):\n",
    "    if '#neveragain' in df4['tweet'].iloc[i]:\n",
    "        again_hash_time.append(df4['created_at'].iloc[i])\n",
    "        \n",
    "idx = pd.DatetimeIndex(again_hash_time)\n",
    "idx_local = idx.tz_localize(tz='UTC')\n",
    "idx_est = idx_local.tz_convert(tz='US/Eastern')\n",
    "idx_est_ds = idx_est - one_hour    #daytime savings adjustment\n",
    "ones = [1]*len(again_hash_time)\n",
    "#ones\n",
    "again_hash = pd.Series(ones, index=idx_est_ds)\n",
    "again_per_minute = again_hash.resample('1min').sum().fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_hash_time =[]\n",
    "\n",
    "for i in range(len(df4['tweet'])):\n",
    "    if '#guncontrol' in df4['tweet'].iloc[i]:\n",
    "        control_hash_time.append(df4['created_at'].iloc[i])\n",
    "        \n",
    "idx = pd.DatetimeIndex(control_hash_time)\n",
    "idx_local = idx.tz_localize(tz='UTC')\n",
    "idx_est = idx_local.tz_convert(tz='US/Eastern')\n",
    "idx_est_ds = idx_est - one_hour    #daytime savings adjustment\n",
    "ones = [1]*len(control_hash_time)\n",
    "#ones\n",
    "control_hash = pd.Series(ones, index=idx_est_ds)\n",
    "control_per_minute = control_hash.resample('1min').sum().fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "violence_hash_time =[]\n",
    "\n",
    "for i in range(len(df4['tweet'])):\n",
    "    if '#gunviolence' in df4['tweet'].iloc[i]:\n",
    "        violence_hash_time.append(df4['created_at'].iloc[i])\n",
    "        \n",
    "idx = pd.DatetimeIndex(violence_hash_time)\n",
    "idx_local = idx.tz_localize(tz='UTC')\n",
    "idx_est = idx_local.tz_convert(tz='US/Eastern')\n",
    "idx_est_ds = idx_est - one_hour    #daytime savings adjustment\n",
    "ones = [1]*len(violence_hash_time)\n",
    "\n",
    "violence_hash = pd.Series(ones, index=idx_est_ds)\n",
    "violence_per_minute = violence_hash.resample('1min').sum().fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "school_hash_time =[]\n",
    "\n",
    "for i in range(len(df4['tweet'])):\n",
    "    if '#schoolshooting' in df4['tweet'].iloc[i]:\n",
    "        school_hash_time.append(df4['created_at'].iloc[i])\n",
    "        \n",
    "idx = pd.DatetimeIndex(school_hash_time)\n",
    "idx_local = idx.tz_localize(tz='UTC')\n",
    "idx_est = idx_local.tz_convert(tz='US/Eastern')\n",
    "idx_est_ds = idx_est - one_hour    #daytime savings adjustment\n",
    "ones = [1]*len(school_hash_time)\n",
    "#ones\n",
    "school_hash = pd.Series(ones, index=idx_est_ds)\n",
    "school_per_minute = school_hash.resample('1min').sum().fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "time_chart = vincent.Line(school_per_minute)\n",
    "time_chart.axis_titles(x='Time', y='Hashtag frequencies')\n",
    "time_chart.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_data = dict(Beach=per_minute, March=march_per_minute, Enough=enough_per_minute, Again=again_per_minute, Control=control_per_minute, Violence=violence_per_minute)\n",
    "all_matches = pd.DataFrame(data = match_data,index=march_per_minute.index)\n",
    "all_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_chart_top3 = vincent.Line(all_matches[['March', 'Enough', 'Again']])\n",
    "time_chart_top3.axis_titles(x='Time', y='Freq')\n",
    "time_chart_top3.legend(title='Top 3 Most popular hashtags')\n",
    "time_chart_top3.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_chart_next3 = vincent.Line(all_matches[['Control', 'Violence', 'Beach']])\n",
    "time_chart_next3.axis_titles(x='Time', y='Freq')\n",
    "time_chart_next3.legend(title='Top 4 to 6 Most popular hashtags')\n",
    "time_chart_next3.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro_guncontrol = []  #collection of words which indictaes the tweet is pro guncontrol\n",
    "anti_guncontrol = []  #collection of words which indicates the tweet is anti guncontrol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.loc[:,'processed_location'] = df4.loc[:,'location'].apply(lambda x: emoji_pattern.sub(r'', x))\n",
    "hashtag = re.compile(r'(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)')\n",
    "number = re.compile(r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)')\n",
    "\n",
    "df4.loc[:,'processed_location2'] = df4.loc[:,'processed_location'].apply(lambda x: hashtag.sub(r'', x))\n",
    "df4.loc[:,'processed_location3'] = df4.loc[:, 'processed_location2'].apply(lambda x: number.sub(r'', x))\n",
    "df4 = df4.drop(['processed_location2', 'processed_location'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4['clean_states'] = df4['processed_location3'].apply(lambda x: re.findall(r'[\\w+],\\s(\\w+[\\s|\\.]*\\w+)', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4['clean_states2'] = df4['processed_location3'].apply(lambda x : re.findall(r'(\\w+[\\s]*[\\w+]+), USA', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.to_csv('location.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:twitter]",
   "language": "python",
   "name": "conda-env-twitter-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
